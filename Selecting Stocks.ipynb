{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# STOCKS DATA\n",
    "df = pd.read_excel('tickersymbols.xls')\t\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering five indexes was proposed of which four are considered \n",
    "# since the nyseUS100 closely follows the nasdaq100,only nasdaq100 is considered to avoid any redundant analysis\n",
    "\n",
    "# Getting all tickers as strings\n",
    "nasdaq100 = list(df['nasdaq100'])\n",
    "nasdaq100Tech = list(df['nasdaq100Tech'])\n",
    "# not considering the missing values in nasdaq100Tech (30 values only as seen in the excel sheet)\n",
    "nasdaq100Tech = nasdaq100Tech[:29] \n",
    "sp100 = list(df['sp100'])\n",
    "# not considering the missing values in sp100 (100 values only as seen in the excel sheet)\n",
    "sp100 = sp100[:99]\n",
    "# removing the formatting letter at front\n",
    "sp100 = [x[1:] for x in sp100]\n",
    "nyseArcaTech100 = list(df['nyseArcaTech100'])\n",
    "# not considering the missing values in nyseArcaTech100 (30 values only as seen in the excel sheet)\n",
    "nyseArcaTech100 = nyseArcaTech100[:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM\n",
      "JNPR\n"
     ]
    }
   ],
   "source": [
    "# taking only those files that are in the dataset\n",
    "\n",
    "files = os.listdir('stock_market_data/nasdaq/csv/')\n",
    "for x in nasdaq100:\n",
    "    if x+'.csv' in files:\n",
    "        shutil.copy('stock_market_data/nasdaq/csv/'+x+'.csv', 'stock_market_data/nasdaq100')\n",
    "        \n",
    "for x in nasdaq100Tech:\n",
    "    if x+'.csv' in files:\n",
    "        #print(x)\n",
    "        shutil.copy('stock_market_data/nasdaq/csv/'+x+'.csv', 'stock_market_data/nasdaq100Tech')\n",
    "\n",
    "files = os.listdir('stock_market_data/sp500/csv/')\n",
    "for x in sp100:\n",
    "    if x+'.csv' in files:\n",
    "        shutil.copy('stock_market_data/sp500/csv/'+x+'.csv', 'stock_market_data/sp100')\n",
    "\n",
    "files = os.listdir('stock_market_data/nyse/csv/')\n",
    "for x in nyseArcaTech100:\n",
    "    if x+'.csv' in files:\n",
    "        shutil.copy('stock_market_data/nyse/csv/'+x+'.csv', 'stock_market_data/nyseArcaTech100')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "20\n",
      "68\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('stock_market_data/nasdaq100')\n",
    "print(len(files))\n",
    "files = os.listdir('stock_market_data/nasdaq100Tech')\n",
    "print(len(files))\n",
    "files = os.listdir('stock_market_data/sp100')\n",
    "print(len(files))\n",
    "files = os.listdir('stock_market_data/nyseArcaTech100')\n",
    "print(len(files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not every stock data is present in the dataset for every stock in the index\n",
    "#    so that will be one drawback of this dataset\n",
    "\n",
    "# Only two files for the nyseArcaTech Index were found\n",
    "# Hence no point in considering this index\n",
    "# Finally, only three indices are considered - nasdaq100, sp100, nasdaq100Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: DO NOT RUN THIS SEGMENT IF YOU ALREADY CREATED THE COMBINED FILE IN THE DIRECTORY. IT WILL COMBINE THAT FILE ALSO.\n",
    "# Combining them all into a single csv file\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "# nasdaq100\n",
    "os.chdir(\"stock_market_data/nasdaq100\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ]).reset_index()\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"nasdaq100.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: DO NOT RUN THIS SEGMENT IF YOU ALREADY CREATED THE COMBINED FILE IN THE DIRECTORY. IT WILL COMBINE THAT FILE ALSO.\n",
    "# sp100\n",
    "os.chdir(\"../sp100\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ]).reset_index()\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"sp100.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: DO NOT RUN THIS SEGMENT IF YOU ALREADY CREATED THE COMBINED FILE IN THE DIRECTORY. IT WILL COMBINE THAT FILE ALSO.\n",
    "# nasdaq100Tech\n",
    "os.chdir(\"../nasdaq100Tech\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ]).reset_index()\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"nasdaq100Tech.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading the data into spark for some basic EDA\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516503\n",
      "54432\n",
      "756\n",
      "{72}\n"
     ]
    }
   ],
   "source": [
    "# nasdaq100\n",
    "df = sqlContext.read.csv('../nasdaq100/nasdaq100.csv',header=True)\n",
    "print(df.rdd.count())\n",
    "# Selecting the data within a time range\n",
    "from datetime import datetime\n",
    "def get_date_time_obj(date_time_str):\n",
    "    date = datetime.strptime(date_time_str, '%d-%m-%Y')\n",
    "    return date\n",
    "min_date = get_date_time_obj('01-07-2018')\n",
    "max_date = get_date_time_obj('01-07-2021')\n",
    "# Use filterbyvalues to select those files in the range\n",
    "filtered_rdd = df.rdd.filter(lambda x: get_date_time_obj(x[1])>=min_date and get_date_time_obj(x[1])<=max_date)\n",
    "print(filtered_rdd.count())\n",
    "# Further EDA\n",
    "# Checking to see if data is present for all 1097 dates from 1st Jul 2018 to 1st Jul 2021\n",
    "# counting by the number of entries for each date\n",
    "value_count_for_each_date = filtered_rdd.map(lambda x: (x[1],1)) \\\n",
    "                    .reduceByKey(lambda a,b: a+b).collect()\n",
    "print(len(value_count_for_each_date))\n",
    "# only 756 values\n",
    "# can be explained as this dataset is not updated daily. Hence, there won't be all 1097 values\n",
    "# but can proceed with this as it has seven data points at regularly for every month and will contain the trend information\n",
    "# a peak into the latest counts\n",
    "value_count_for_each_date[-15:]\n",
    "# checking for missing entries for any stock\n",
    "counts = set([x[1] for x in value_count_for_each_date ])\n",
    "print(counts)\n",
    "# 72 is the only unique count value\n",
    "# So 756 values present for all 72 stocks. No missing entries for any date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index             0\n",
       "Date              0\n",
       "Low               0\n",
       "Open              0\n",
       "Volume            0\n",
       "High              0\n",
       "Close             0\n",
       "Adjusted Close    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for any other missing values\n",
    "df = sqlContext.createDataFrame(filtered_rdd)\n",
    "print(df.na.drop().rdd.count())\n",
    "# counts before and after removing missing values is the same\n",
    "# Hence no missing values\n",
    "# also cross checking for any missing values with pandas\n",
    "df_pd = pd.read_csv('../nasdaq100/nasdaq100.csv')\n",
    "df_pd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file\n",
    "os.chdir('../nasdaq100')\n",
    "df.write.csv('../nasdaq100/nasdaq100_final')\n",
    "!cat nasdaq100_final/part* > nasdaq100_final.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639414\n",
      "51408\n",
      "756\n",
      "{68}\n"
     ]
    }
   ],
   "source": [
    "# sp100\n",
    "df = sqlContext.read.csv('../sp100/sp100.csv',header=True)\n",
    "print(df.rdd.count())\n",
    "# Selecting the data within a time range\n",
    "from datetime import datetime\n",
    "def get_date_time_obj(date_time_str):\n",
    "    date = datetime.strptime(date_time_str, '%d-%m-%Y')\n",
    "    return date\n",
    "min_date = get_date_time_obj('01-07-2018')\n",
    "max_date = get_date_time_obj('01-07-2021')\n",
    "# Use filterbyvalues to select those files in the range\n",
    "filtered_rdd = df.rdd.filter(lambda x: get_date_time_obj(x[1])>=min_date and get_date_time_obj(x[1])<=max_date)\n",
    "print(filtered_rdd.count())\n",
    "# Further EDA\n",
    "# Checking to see if data is present for all 1097 dates from 1st Jul 2018 to 1st Jul 2021\n",
    "# counting by the number of entries for each date\n",
    "value_count_for_each_date = filtered_rdd.map(lambda x: (x[1],1)) \\\n",
    "                    .reduceByKey(lambda a,b: a+b).collect()\n",
    "print(len(value_count_for_each_date))\n",
    "# similarly, only 756 values\n",
    "# a peak into the latest counts\n",
    "value_count_for_each_date[-15:]\n",
    "# checking for missing entries for any stock\n",
    "counts = set([x[1] for x in value_count_for_each_date ])\n",
    "print(counts)\n",
    "# 68 is the only unique count value\n",
    "# So 756 values present for all 68 stocks. No missing entries for any date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index             0\n",
       "Date              0\n",
       "Low               0\n",
       "Open              0\n",
       "Volume            0\n",
       "High              0\n",
       "Close             0\n",
       "Adjusted Close    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for any other missing values\n",
    "df = sqlContext.createDataFrame(filtered_rdd)\n",
    "print(df.na.drop().rdd.count())\n",
    "# counts before and after removing missing values is the same\n",
    "# Hence no missing values\n",
    "# also cross checking for any missing values with pandas\n",
    "df_pd = pd.read_csv('../sp100/sp100.csv')\n",
    "df_pd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file\n",
    "os.chdir('../sp100')\n",
    "df.write.csv('../sp100/sp100_final')\n",
    "!cat sp100_final/part* > sp100_final.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156839\n",
      "15120\n",
      "756\n",
      "{20}\n"
     ]
    }
   ],
   "source": [
    "# nasdaq100Tech\n",
    "df = sqlContext.read.csv('../nasdaq100Tech/nasdaq100Tech.csv',header=True)\n",
    "print(df.rdd.count())\n",
    "# Selecting the data within a time range\n",
    "from datetime import datetime\n",
    "def get_date_time_obj(date_time_str):\n",
    "    date = datetime.strptime(date_time_str, '%d-%m-%Y')\n",
    "    return date\n",
    "min_date = get_date_time_obj('01-07-2018')\n",
    "max_date = get_date_time_obj('01-07-2021')\n",
    "# Use filterbyvalues to select those files in the range\n",
    "filtered_rdd = df.rdd.filter(lambda x: get_date_time_obj(x[1])>=min_date and get_date_time_obj(x[1])<=max_date)\n",
    "print(filtered_rdd.count())\n",
    "# Further EDA\n",
    "# Checking to see if data is present for all 1097 dates from 1st Jul 2018 to 1st Jul 2021\n",
    "# counting by the number of entries for each date\n",
    "value_count_for_each_date = filtered_rdd.map(lambda x: (x[1],1)) \\\n",
    "                    .reduceByKey(lambda a,b: a+b).collect()\n",
    "print(len(value_count_for_each_date))\n",
    "# similarly, only 756 values\n",
    "# a peak into the latest counts\n",
    "value_count_for_each_date[-15:]\n",
    "# checking for missing entries for any stock\n",
    "counts = set([x[1] for x in value_count_for_each_date ])\n",
    "print(counts)\n",
    "# 20 is the only unique count value\n",
    "# So 756 values present for all 20 stocks. No missing entries for any date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index             0\n",
       "Date              0\n",
       "Low               0\n",
       "Open              0\n",
       "Volume            0\n",
       "High              0\n",
       "Close             0\n",
       "Adjusted Close    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for any other missing values\n",
    "df = sqlContext.createDataFrame(filtered_rdd)\n",
    "print(df.na.drop().rdd.count())\n",
    "# counts before and after removing missing values is the same\n",
    "# Hence no missing values\n",
    "# also cross checking for any missing values with pandas\n",
    "df_pd = pd.read_csv('../nasdaq100Tech/nasdaq100Tech.csv')\n",
    "df_pd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file\n",
    "os.chdir('../nasdaq100Tech')\n",
    "df.write.csv('../nasdaq100Tech/nasdaq100Tech_final')\n",
    "!cat nasdaq100Tech_final/part* > nasdaq100Tech_final.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
