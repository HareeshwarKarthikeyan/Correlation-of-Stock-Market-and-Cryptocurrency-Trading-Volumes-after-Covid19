{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# STOCKS DATA\n",
    "df = pd.read_excel('tickersymbols.xls')\t\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Considering five indexes was proposed of which four are considered \n",
    "# since the nyseUS100 closely follows the nasdaq100,only nasdaq100 is considered to avoid any redundant analysis\n",
    "\n",
    "# Getting all tickers as strings\n",
    "nasdaq100 = list(df['nasdaq100'])\n",
    "nasdaq100Tech = list(df['nasdaq100Tech'])\n",
    "# not considering the missing values in nasdaq100Tech (30 values only as seen in the excel sheet)\n",
    "nasdaq100Tech = nasdaq100Tech[:29] \n",
    "sp100 = list(df['sp100'])\n",
    "# not considering the missing values in sp100 (100 values only as seen in the excel sheet)\n",
    "sp100 = sp100[:99]\n",
    "# removing the formatting letter at front\n",
    "sp100 = [x[1:] for x in sp100]\n",
    "nyseArcaTech100 = list(df['nyseArcaTech100'])\n",
    "# not considering the missing values in nyseArcaTech100 (30 values only as seen in the excel sheet)\n",
    "nyseArcaTech100 = nyseArcaTech100[:29]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IBM\n",
      "JNPR\n"
     ]
    }
   ],
   "source": [
    "# taking only those files that are in the dataset\n",
    "\n",
    "files = os.listdir('stock_market_data/nasdaq/csv/')\n",
    "for x in nasdaq100:\n",
    "    if x+'.csv' in files:\n",
    "        shutil.copy('stock_market_data/nasdaq/csv/'+x+'.csv', 'stock_market_data/nasdaq100')\n",
    "        \n",
    "for x in nasdaq100Tech:\n",
    "    if x+'.csv' in files:\n",
    "        #print(x)\n",
    "        shutil.copy('stock_market_data/nasdaq/csv/'+x+'.csv', 'stock_market_data/nasdaq100Tech')\n",
    "\n",
    "files = os.listdir('stock_market_data/sp500/csv/')\n",
    "for x in sp100:\n",
    "    if x+'.csv' in files:\n",
    "        shutil.copy('stock_market_data/sp500/csv/'+x+'.csv', 'stock_market_data/sp100')\n",
    "\n",
    "files = os.listdir('stock_market_data/nyse/csv/')\n",
    "for x in nyseArcaTech100:\n",
    "    if x+'.csv' in files:\n",
    "        shutil.copy('stock_market_data/nyse/csv/'+x+'.csv', 'stock_market_data/nyseArcaTech100')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "20\n",
      "68\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('stock_market_data/nasdaq100')\n",
    "print(len(files))\n",
    "files = os.listdir('stock_market_data/nasdaq100Tech')\n",
    "print(len(files))\n",
    "files = os.listdir('stock_market_data/sp100')\n",
    "print(len(files))\n",
    "files = os.listdir('stock_market_data/nyseArcaTech100')\n",
    "print(len(files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not every stock data is present in the dataset for every stock in the index\n",
    "#    so that will be one drawback of this dataset\n",
    "\n",
    "# Only two files for the nyseArcaTech Index were found\n",
    "# Hence no point in considering this index\n",
    "# Finally, only three indices are considered - nasdaq100, sp100, nasdaq100Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/harry/Desktop/Correlation-of-Stock-Market-and-Cryptocurrency-Trading-Volumes-after-Covid19/stock_market_data/sp100\n"
     ]
    }
   ],
   "source": [
    "#os.chdir(\"Desktop/Correlation-of-Stock-Market-and-Cryptocurrency-Trading-Volumes-after-Covid19\")\n",
    "os.chdir(\"stock_market_data/sp100\")\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: DO NOT RUN THIS SEGMENT IF YOU ALREADY CREATED THE COMBINED FILE IN THE DIRECTORY. IT WILL COMBINE THAT FILE ALSO.\n",
    "# Combining them all into a single csv file\n",
    "# Doesn't involve any large scale computation yet. So using pandas to quickly finish this step\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "# nasdaq100\n",
    "os.chdir(\"stock_market_data/nasdaq100\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "# Adding symbol column for each file\n",
    "for filename in all_filenames:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['SYMBOL'] = filename[:-4]\n",
    "    df.to_csv(filename)\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ]).reset_index()\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"nasdaq100.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: DO NOT RUN THIS SEGMENT IF YOU ALREADY CREATED THE COMBINED FILE IN THE DIRECTORY. IT WILL COMBINE THAT FILE ALSO.\n",
    "# sp100\n",
    "os.chdir(\"../sp100\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "# Adding symbol column for each file\n",
    "for filename in all_filenames:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['SYMBOL'] = filename[:-4]\n",
    "    df.to_csv(filename)\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ]).reset_index()\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"sp100.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTE: DO NOT RUN THIS SEGMENT IF YOU ALREADY CREATED THE COMBINED FILE IN THE DIRECTORY. IT WILL COMBINE THAT FILE ALSO.\n",
    "# nasdaq100Tech\n",
    "os.chdir(\"../nasdaq100Tech\")\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "# Adding symbol column for each file\n",
    "for filename in all_filenames:\n",
    "    df = pd.read_csv(filename)\n",
    "    df['SYMBOL'] = filename[:-4]\n",
    "    df.to_csv(filename)\n",
    "#combine all files in the list\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ]).reset_index()\n",
    "#export to csv\n",
    "combined_csv.to_csv( \"nasdaq100Tech.csv\", index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reading the data into spark for some basic EDA\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "516503\n",
      "54432\n",
      "756\n",
      "{72}\n"
     ]
    }
   ],
   "source": [
    "# nasdaq100\n",
    "df = sqlContext.read.csv('../nasdaq100/nasdaq100.csv',header=True)\n",
    "print(df.rdd.count())\n",
    "# Selecting the data within a time range\n",
    "from datetime import datetime\n",
    "def get_date_time_obj(date_time_str):\n",
    "    date = datetime.strptime(date_time_str, '%d-%m-%Y')\n",
    "    return date\n",
    "min_date = get_date_time_obj('01-07-2018')\n",
    "max_date = get_date_time_obj('01-07-2021')\n",
    "# Use filterbyvalues to select those files in the range\n",
    "filtered_rdd = df.rdd.filter(lambda x: get_date_time_obj(x[2])>=min_date and get_date_time_obj(x[2])<=max_date)\n",
    "print(filtered_rdd.count())\n",
    "# Further EDA\n",
    "# Checking to see if data is present for all 1097 dates from 1st Jul 2018 to 1st Jul 2021\n",
    "# counting by the number of entries for each date\n",
    "value_count_for_each_date = filtered_rdd.map(lambda x: (x[2],1)) \\\n",
    "                    .reduceByKey(lambda a,b: a+b).collect()\n",
    "print(len(value_count_for_each_date))\n",
    "# only 756 values\n",
    "# can be explained as this dataset is not updated daily as the stock market is closed on weekends and on other holidays. Hence, there won't be all 1097 values\n",
    "# but can proceed with this as it has seven data points at regularly for every month and will contain the trend information\n",
    "# a peak into the latest counts\n",
    "value_count_for_each_date[-15:]\n",
    "# checking for missing entries for any stock\n",
    "counts = set([x[1] for x in value_count_for_each_date ])\n",
    "print(counts)\n",
    "# 72 is the only unique count value\n",
    "# So 756 values present for all 72 stocks. No missing entries for any date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54432\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index             0\n",
       "Unnamed: 0        0\n",
       "Date              0\n",
       "Low               0\n",
       "Open              0\n",
       "Volume            0\n",
       "High              0\n",
       "Close             0\n",
       "Adjusted Close    0\n",
       "SYMBOL            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for any other missing values\n",
    "df = sqlContext.createDataFrame(filtered_rdd)\n",
    "print(df.na.drop().rdd.count())\n",
    "# counts before and after removing missing values is the same\n",
    "# Hence no missing values\n",
    "# also cross checking for any missing values with pandas\n",
    "df_pd = pd.read_csv('../nasdaq100/nasdaq100.csv')\n",
    "df_pd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/Users/harry/Desktop/Correlation-of-Stock-Market-and-Cryptocurrency-Trading-Volumes-after-Covid19/stock_market_data/nasdaq100/nasdaq100_final already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-0e8e931bb661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# saving the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../nasdaq100'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../nasdaq100/nasdaq100_final'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cat nasdaq100_final/part* > nasdaq100_final.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1370\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/Users/harry/Desktop/Correlation-of-Stock-Market-and-Cryptocurrency-Trading-Volumes-after-Covid19/stock_market_data/nasdaq100/nasdaq100_final already exists."
     ]
    }
   ],
   "source": [
    "# saving the file\n",
    "os.chdir('../nasdaq100')\n",
    "df.write.csv('../nasdaq100/nasdaq100_final')\n",
    "!cat nasdaq100_final/part* > nasdaq100_final.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639414\n",
      "51408\n",
      "756\n",
      "{68}\n"
     ]
    }
   ],
   "source": [
    "# sp100\n",
    "df = sqlContext.read.csv('../sp100/sp100.csv',header=True)\n",
    "print(df.rdd.count())\n",
    "# Selecting the data within a time range\n",
    "from datetime import datetime\n",
    "def get_date_time_obj(date_time_str):\n",
    "    date = datetime.strptime(date_time_str, '%d-%m-%Y')\n",
    "    return date\n",
    "min_date = get_date_time_obj('01-07-2018')\n",
    "max_date = get_date_time_obj('01-07-2021')\n",
    "# Use filterbyvalues to select those files in the range\n",
    "filtered_rdd = df.rdd.filter(lambda x: get_date_time_obj(x[2])>=min_date and get_date_time_obj(x[2])<=max_date)\n",
    "print(filtered_rdd.count())\n",
    "# Further EDA\n",
    "# Checking to see if data is present for all dates from 1st Jul 2018 to 1st Jul 2021\n",
    "# counting by the number of entries for each date\n",
    "value_count_for_each_date = filtered_rdd.map(lambda x: (x[2],1)) \\\n",
    "                    .reduceByKey(lambda a,b: a+b).collect()\n",
    "print(len(value_count_for_each_date))\n",
    "# similarly, only 756 values\n",
    "# a peak into the latest counts\n",
    "value_count_for_each_date[-15:]\n",
    "# checking for missing entries for any stock\n",
    "counts = set([x[1] for x in value_count_for_each_date ])\n",
    "print(counts)\n",
    "# 68 is the only unique count value\n",
    "# So 756 values present for all 68 stocks. No missing entries for any date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index             0\n",
       "Unnamed: 0        0\n",
       "Date              0\n",
       "Low               0\n",
       "Open              0\n",
       "Volume            0\n",
       "High              0\n",
       "Close             0\n",
       "Adjusted Close    0\n",
       "SYMBOL            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for any other missing values\n",
    "df = sqlContext.createDataFrame(filtered_rdd)\n",
    "print(df.na.drop().rdd.count())\n",
    "# counts before and after removing missing values is the same\n",
    "# Hence no missing values\n",
    "# also cross checking for any missing values with pandas\n",
    "df_pd = pd.read_csv('../sp100/sp100.csv')\n",
    "df_pd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file\n",
    "os.chdir('../sp100')\n",
    "df.write.csv('../sp100/sp100_final')\n",
    "!cat sp100_final/part* > sp100_final.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156839\n",
      "15120\n",
      "756\n",
      "{20}\n"
     ]
    }
   ],
   "source": [
    "# nasdaq100Tech\n",
    "df = sqlContext.read.csv('../nasdaq100Tech/nasdaq100Tech.csv',header=True)\n",
    "print(df.rdd.count())\n",
    "# Selecting the data within a time range\n",
    "from datetime import datetime\n",
    "def get_date_time_obj(date_time_str):\n",
    "    date = datetime.strptime(date_time_str, '%d-%m-%Y')\n",
    "    return date\n",
    "min_date = get_date_time_obj('01-07-2018')\n",
    "max_date = get_date_time_obj('01-07-2021')\n",
    "# Use filterbyvalues to select those files in the range\n",
    "filtered_rdd = df.rdd.filter(lambda x: get_date_time_obj(x[2])>=min_date and get_date_time_obj(x[2])<=max_date)\n",
    "print(filtered_rdd.count())\n",
    "# Further EDA\n",
    "# Checking to see if data is present for all dates from 1st Jul 2018 to 1st Jul 2021\n",
    "# counting by the number of entries for each date\n",
    "value_count_for_each_date = filtered_rdd.map(lambda x: (x[2],1)) \\\n",
    "                    .reduceByKey(lambda a,b: a+b).collect()\n",
    "print(len(value_count_for_each_date))\n",
    "# similarly, only 756 values\n",
    "# a peak into the latest counts\n",
    "value_count_for_each_date[-15:]\n",
    "# checking for missing entries for any stock\n",
    "counts = set([x[1] for x in value_count_for_each_date ])\n",
    "print(counts)\n",
    "# 20 is the only unique count value\n",
    "# So 756 values present for all 20 stocks. No missing entries for any date!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15120\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "index             0\n",
       "Unnamed: 0        0\n",
       "Date              0\n",
       "Low               0\n",
       "Open              0\n",
       "Volume            0\n",
       "High              0\n",
       "Close             0\n",
       "Adjusted Close    0\n",
       "SYMBOL            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for any other missing values\n",
    "df = sqlContext.createDataFrame(filtered_rdd)\n",
    "print(df.na.drop().rdd.count())\n",
    "# counts before and after removing missing values is the same\n",
    "# Hence no missing values\n",
    "# also cross checking for any missing values with pandas\n",
    "df_pd = pd.read_csv('../nasdaq100Tech/nasdaq100Tech.csv')\n",
    "df_pd.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the file\n",
    "os.chdir('../nasdaq100Tech')\n",
    "df.write.csv('../nasdaq100Tech/nasdaq100Tech_final')\n",
    "!cat nasdaq100Tech_final/part* > nasdaq100Tech_final.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
